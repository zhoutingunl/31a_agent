"""
æ‰¹è¯„è€…æ¨¡å— - è¯„ä¼°Agentè¾“å‡ºè´¨é‡

åŠŸèƒ½ï¼š
- ä½¿ç”¨LLMè¯„ä¼°Agentè¾“å‡º
- è¯†åˆ«é”™è¯¯å’Œé—®é¢˜
- æä¾›å»ºè®¾æ€§åé¦ˆ
- åˆ¤æ–­æ˜¯å¦éœ€è¦çº é”™
"""

import json
import re
from typing import List, Dict, Any, Optional
from datetime import datetime

from app.core.llm.base import BaseLLM
from app.core.reflection.schemas import (
    CriticFeedback, ExecutionContext, QualityDimension, QualityScore
)
from app.core.reflection.quality_scorer import QualityScorer
from app.utils.logger import get_logger

logger = get_logger(__name__)


class Critic:
    """
    æ‰¹è¯„è€…
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨LLMè¯„ä¼°Agentè¾“å‡º
    - è¯†åˆ«é”™è¯¯å’Œé—®é¢˜
    - æä¾›å»ºè®¾æ€§åé¦ˆ
    - åˆ¤æ–­æ˜¯å¦éœ€è¦çº é”™
    """
    
    def __init__(self, llm: BaseLLM):
        """
        åˆå§‹åŒ–æ‰¹è¯„è€…
        
        å‚æ•°:
            llm: LLMå®ä¾‹
        """
        self.llm = llm
        self.quality_scorer = QualityScorer(llm)
        self.logger = get_logger(f"{__name__}.{self.__class__.__name__}")
        
        # æ‰¹è¯„è€…æç¤ºè¯
        self.critic_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æ‰¹è¯„è€…ï¼Œè´Ÿè´£è¯„ä¼°AI Agentçš„è¾“å‡ºè´¨é‡ã€‚

ä»»åŠ¡æè¿°: {task_description}
æœŸæœ›ç›®æ ‡: {expected_goal}
Agentè¾“å‡º: {agent_output}
çº¦æŸæ¡ä»¶: {constraints}

è¯·ä»ä»¥ä¸‹ç»´åº¦ä¸¥æ ¼è¯„ä¼°è¾“å‡ºè´¨é‡ï¼š

1. **æ­£ç¡®æ€§ (Correctness)**: 
   - è¾“å‡ºæ˜¯å¦æ­£ç¡®è§£å†³äº†é—®é¢˜ï¼Ÿ
   - æ˜¯å¦åŒ…å«é€»è¾‘é”™è¯¯ã€è¯­æ³•é”™è¯¯æˆ–äº‹å®é”™è¯¯ï¼Ÿ
   - æ˜¯å¦éµå¾ªäº†æœ€ä½³å®è·µï¼Ÿ

2. **å®Œæ•´æ€§ (Completeness)**:
   - æ˜¯å¦æ¶µç›–äº†æ‰€æœ‰è¦æ±‚ï¼Ÿ
   - æ˜¯å¦é—æ¼äº†é‡è¦ä¿¡æ¯æˆ–æ­¥éª¤ï¼Ÿ
   - æ˜¯å¦æä¾›äº†å®Œæ•´çš„è§£å†³æ–¹æ¡ˆï¼Ÿ

3. **æ•ˆç‡ (Efficiency)**:
   - è§£å†³æ–¹æ¡ˆæ˜¯å¦é«˜æ•ˆï¼Ÿ
   - æ˜¯å¦ä½¿ç”¨äº†æœ€ä¼˜çš„æ–¹æ³•ï¼Ÿ
   - æ˜¯å¦æœ‰ä¸å¿…è¦çš„å†—ä½™ï¼Ÿ

4. **æ¸…æ™°åº¦ (Clarity)**:
   - è¾“å‡ºæ˜¯å¦æ˜“äºç†è§£ï¼Ÿ
   - ç»“æ„æ˜¯å¦æ¸…æ™°ï¼Ÿ
   - è¯­è¨€æ˜¯å¦å‡†ç¡®ï¼Ÿ

è¯·ä»¥JSONæ ¼å¼è¿”å›è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼š
{{
    "overall_score": 0.75,
    "dimension_scores": [
        {{
            "dimension": "correctness",
            "score": 0.8,
            "explanation": "åŸºæœ¬æ­£ç¡®ï¼Œä½†å­˜åœ¨ä¸€ä¸ªå°é”™è¯¯..."
        }},
        {{
            "dimension": "completeness",
            "score": 0.7,
            "explanation": "æ¶µç›–äº†ä¸»è¦è¦æ±‚ï¼Œä½†ç¼ºå°‘..."
        }},
        {{
            "dimension": "efficiency", 
            "score": 0.8,
            "explanation": "è§£å†³æ–¹æ¡ˆåˆç†ï¼Œä½†å¯ä»¥ä¼˜åŒ–..."
        }},
        {{
            "dimension": "clarity",
            "score": 0.7,
            "explanation": "ç»“æ„æ¸…æ™°ï¼Œä½†æŸäº›éƒ¨åˆ†å¯ä»¥æ›´ç®€æ´..."
        }}
    ],
    "issues": [
        "å…·ä½“é—®é¢˜1ï¼šæè¿°é—®é¢˜è¯¦æƒ…",
        "å…·ä½“é—®é¢˜2ï¼šæè¿°é—®é¢˜è¯¦æƒ…"
    ],
    "strengths": [
        "ä¼˜ç‚¹1ï¼šåšå¾—å¥½çš„åœ°æ–¹",
        "ä¼˜ç‚¹2ï¼šåšå¾—å¥½çš„åœ°æ–¹"
    ],
    "needs_correction": true,
    "correction_priority": "high|medium|low",
    "detailed_feedback": "è¯¦ç»†çš„åé¦ˆæ–‡æœ¬ï¼ŒåŒ…æ‹¬å…·ä½“çš„æ”¹è¿›å»ºè®®"
}}

è¯·ç¡®ä¿ï¼š
1. è¯„åˆ†å®¢è§‚å…¬æ­£ï¼ŒåŸºäºäº‹å®è€Œéä¸»è§‚åˆ¤æ–­
2. é—®é¢˜æè¿°å…·ä½“æ˜ç¡®ï¼Œä¾¿äºæ”¹è¿›
3. æä¾›å»ºè®¾æ€§çš„æ”¹è¿›å»ºè®®
4. è¯†åˆ«è¾“å‡ºä¸­çš„çœŸæ­£é—®é¢˜ï¼Œé¿å…è¿‡åº¦æ‰¹è¯„"""

    async def evaluate(self, 
                      output: str, 
                      context: ExecutionContext) -> CriticFeedback:
        """
        è¯„ä¼°Agentè¾“å‡º
        
        å‚æ•°:
            output: Agentè¾“å‡ºå†…å®¹
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        è¿”å›:
            CriticFeedback: æ‰¹è¯„åé¦ˆ
        """
        try:
            self.logger.info(f"å¼€å§‹æ‰¹è¯„è¯„ä¼°ï¼Œä»»åŠ¡: {context.task_description[:50]}...")
            
            # 1. ä½¿ç”¨è´¨é‡è¯„åˆ†å™¨è¿›è¡ŒåŸºç¡€è¯„ä¼°
            quality_feedback = await self.quality_scorer.score_output(output, context)
            
            # 2. ä½¿ç”¨LLMè¿›è¡Œæ·±åº¦æ‰¹è¯„è¯„ä¼°
            critic_feedback = await self._llm_critic_evaluation(output, context)
            
            # 3. ç»¼åˆä¸¤ç§è¯„ä¼°ç»“æœ
            final_feedback = self._combine_evaluations(quality_feedback, critic_feedback, output, context)
            
            self.logger.info(f"æ‰¹è¯„è¯„ä¼°å®Œæˆï¼Œæ€»åˆ†: {final_feedback.overall_score:.2f}, éœ€è¦çº é”™: {final_feedback.needs_correction}")
            return final_feedback
            
        except Exception as e:
            self.logger.error(f"æ‰¹è¯„è¯„ä¼°å¤±è´¥: {e}")
            return self._create_error_feedback(output, context, str(e))

    async def _llm_critic_evaluation(self, 
                                   output: str, 
                                   context: ExecutionContext) -> Dict[str, Any]:
        """
        ä½¿ç”¨LLMè¿›è¡Œæ‰¹è¯„è¯„ä¼°
        
        å‚æ•°:
            output: è¾“å‡ºå†…å®¹
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        è¿”å›:
            Dict[str, Any]: LLMè¯„ä¼°ç»“æœ
        """
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self.critic_prompt.format(
                task_description=context.task_description,
                expected_goal=context.expected_goal,
                agent_output=output,
                constraints=", ".join(context.constraints) if context.constraints else "æ— "
            )
            
            # è°ƒç”¨LLM
            response = await self.llm.achat(prompt)
            
            # è§£æJSONå“åº”
            try:
                result = json.loads(response)
                return result
            except json.JSONDecodeError:
                # å°è¯•æå–JSONéƒ¨åˆ†
                json_match = re.search(r'\{.*\}', response, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    return result
                else:
                    raise ValueError("æ— æ³•è§£æLLMå“åº”ä¸ºJSON")
                    
        except Exception as e:
            self.logger.error(f"LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥: {e}")
            return self._create_fallback_critic_result()

    def _combine_evaluations(self, 
                           quality_feedback: CriticFeedback,
                           critic_result: Dict[str, Any],
                           output: str,
                           context: ExecutionContext) -> CriticFeedback:
        """
        ç»¼åˆè´¨é‡è¯„åˆ†å’Œæ‰¹è¯„è¯„ä¼°
        
        å‚æ•°:
            quality_feedback: è´¨é‡è¯„åˆ†åé¦ˆ
            critic_result: æ‰¹è¯„è¯„ä¼°ç»“æœ
            output: è¾“å‡ºå†…å®¹
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        è¿”å›:
            CriticFeedback: ç»¼åˆåé¦ˆ
        """
        # æå–æ‰¹è¯„è¯„ä¼°çš„ç»´åº¦è¯„åˆ†
        critic_dimension_scores = []
        if "dimension_scores" in critic_result:
            for dim_score in critic_result["dimension_scores"]:
                dimension = dim_score.get("dimension", "")
                score = dim_score.get("score", 0.5)
                explanation = dim_score.get("explanation", "")
                
                try:
                    critic_dimension_scores.append(QualityScore(
                        dimension=QualityDimension(dimension),
                        score=score,
                        explanation=explanation
                    ))
                except ValueError:
                    # å¦‚æœç»´åº¦åç§°æ— æ•ˆï¼Œè·³è¿‡
                    continue
        
        # ç»¼åˆè¯„åˆ†ï¼ˆè´¨é‡è¯„åˆ†æƒé‡0.4ï¼Œæ‰¹è¯„è¯„ä¼°æƒé‡0.6ï¼‰
        final_dimension_scores = []
        for quality_score in quality_feedback.dimension_scores:
            # æŸ¥æ‰¾å¯¹åº”çš„æ‰¹è¯„è¯„åˆ†
            critic_score = None
            for cs in critic_dimension_scores:
                if cs.dimension == quality_score.dimension:
                    critic_score = cs
                    break
            
            if critic_score:
                # ç»¼åˆè¯„åˆ†
                final_score = quality_score.score * 0.4 + critic_score.score * 0.6
                final_explanation = f"è´¨é‡è¯„åˆ†: {quality_score.score:.2f}, æ‰¹è¯„è¯„åˆ†: {critic_score.score:.2f}. {critic_score.explanation}"
            else:
                # å¦‚æœæ²¡æœ‰æ‰¹è¯„è¯„åˆ†ï¼Œä½¿ç”¨è´¨é‡è¯„åˆ†
                final_score = quality_score.score
                final_explanation = quality_score.explanation
            
            final_dimension_scores.append(QualityScore(
                dimension=quality_score.dimension,
                score=final_score,
                explanation=final_explanation
            ))
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        overall_score = sum(score.score for score in final_dimension_scores) / len(final_dimension_scores)
        
        # åˆå¹¶é—®é¢˜å’Œä¼˜ç‚¹
        issues = list(set(quality_feedback.issues + critic_result.get("issues", [])))
        strengths = list(set(quality_feedback.strengths + critic_result.get("strengths", [])))
        
        # åˆ¤æ–­æ˜¯å¦éœ€è¦çº é”™
        needs_correction = (
            quality_feedback.needs_correction or 
            critic_result.get("needs_correction", False) or
            overall_score < 0.8
        )
        
        # ç”Ÿæˆç»¼åˆåé¦ˆæ–‡æœ¬
        feedback_text = self._generate_comprehensive_feedback(
            overall_score, final_dimension_scores, issues, strengths, critic_result
        )
        
        return CriticFeedback(
            overall_score=overall_score,
            dimension_scores=final_dimension_scores,
            issues=issues,
            strengths=strengths,
            needs_correction=needs_correction,
            feedback_text=feedback_text
        )

    def _generate_comprehensive_feedback(self,
                                       overall_score: float,
                                       dimension_scores: List[QualityScore],
                                       issues: List[str],
                                       strengths: List[str],
                                       critic_result: Dict[str, Any]) -> str:
        """ç”Ÿæˆç»¼åˆåé¦ˆæ–‡æœ¬"""
        feedback_parts = []
        
        # æ€»ä½“è¯„åˆ†
        feedback_parts.append(f"ğŸ“Š æ€»ä½“è¯„åˆ†: {overall_score:.2f}/1.0")
        
        # å„ç»´åº¦è¯„åˆ†
        feedback_parts.append("\nğŸ“ˆ å„ç»´åº¦è¯„åˆ†:")
        for score in dimension_scores:
            emoji = self._get_dimension_emoji(score.dimension)
            feedback_parts.append(f"{emoji} {score.dimension.value}: {score.score:.2f} - {score.explanation}")
        
        # ä¼˜ç‚¹
        if strengths:
            feedback_parts.append(f"\nâœ… ä¼˜ç‚¹:")
            for strength in strengths:
                feedback_parts.append(f"  â€¢ {strength}")
        
        # é—®é¢˜
        if issues:
            feedback_parts.append(f"\nâŒ éœ€è¦æ”¹è¿›çš„é—®é¢˜:")
            for issue in issues:
                feedback_parts.append(f"  â€¢ {issue}")
        
        # è¯¦ç»†åé¦ˆ
        if "detailed_feedback" in critic_result:
            feedback_parts.append(f"\nğŸ’¡ è¯¦ç»†å»ºè®®:")
            feedback_parts.append(critic_result["detailed_feedback"])
        
        # çº é”™ä¼˜å…ˆçº§
        if "correction_priority" in critic_result:
            priority = critic_result["correction_priority"]
            priority_emoji = {"high": "ğŸ”´", "medium": "ğŸŸ¡", "low": "ğŸŸ¢"}.get(priority, "âšª")
            feedback_parts.append(f"\n{priority_emoji} çº é”™ä¼˜å…ˆçº§: {priority}")
        
        return "\n".join(feedback_parts)

    def _get_dimension_emoji(self, dimension: QualityDimension) -> str:
        """è·å–ç»´åº¦å¯¹åº”çš„emoji"""
        emoji_map = {
            QualityDimension.CORRECTNESS: "ğŸ¯",
            QualityDimension.COMPLETENESS: "ğŸ“‹",
            QualityDimension.EFFICIENCY: "âš¡",
            QualityDimension.CLARITY: "ğŸ’¡"
        }
        return emoji_map.get(dimension, "ğŸ“Š")

    def _create_error_feedback(self, output: str, context: ExecutionContext, error: str) -> CriticFeedback:
        """åˆ›å»ºé”™è¯¯åé¦ˆï¼ˆå½“è¯„ä¼°å¤±è´¥æ—¶ï¼‰"""
        return CriticFeedback(
            overall_score=0.2,
            dimension_scores=[
                QualityScore(
                    dimension=QualityDimension.CORRECTNESS,
                    score=0.2,
                    explanation=f"è¯„ä¼°å¤±è´¥: {error}"
                ),
                QualityScore(
                    dimension=QualityDimension.COMPLETENESS,
                    score=0.2,
                    explanation=f"è¯„ä¼°å¤±è´¥: {error}"
                ),
                QualityScore(
                    dimension=QualityDimension.EFFICIENCY,
                    score=0.2,
                    explanation=f"è¯„ä¼°å¤±è´¥: {error}"
                ),
                QualityScore(
                    dimension=QualityDimension.CLARITY,
                    score=0.2,
                    explanation=f"è¯„ä¼°å¤±è´¥: {error}"
                )
            ],
            issues=[f"æ‰¹è¯„è¯„ä¼°å¤±è´¥: {error}"],
            strengths=[],
            needs_correction=True,
            feedback_text=f"ç”±äºæ‰¹è¯„è¯„ä¼°è¿‡ç¨‹å‡ºé”™ï¼Œæ— æ³•å‡†ç¡®è¯„ä¼°è¾“å‡ºè´¨é‡ã€‚é”™è¯¯: {error}"
        )

    def _create_fallback_critic_result(self) -> Dict[str, Any]:
        """åˆ›å»ºæ‰¹è¯„è¯„ä¼°çš„å¤‡ç”¨ç»“æœ"""
        return {
            "overall_score": 0.5,
            "dimension_scores": [
                {"dimension": "correctness", "score": 0.5, "explanation": "LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"},
                {"dimension": "completeness", "score": 0.5, "explanation": "LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"},
                {"dimension": "efficiency", "score": 0.5, "explanation": "LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"},
                {"dimension": "clarity", "score": 0.5, "explanation": "LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„åˆ†"}
            ],
            "issues": ["LLMæ‰¹è¯„è¯„ä¼°å¤±è´¥"],
            "strengths": [],
            "needs_correction": True,
            "correction_priority": "medium",
            "detailed_feedback": "ç”±äºLLMè¯„ä¼°å¤±è´¥ï¼Œæ— æ³•æä¾›è¯¦ç»†çš„æ‰¹è¯„åé¦ˆã€‚"
        }

    async def quick_evaluate(self, output: str, context: ExecutionContext) -> bool:
        """
        å¿«é€Ÿè¯„ä¼°æ˜¯å¦éœ€è¦çº é”™
        
        å‚æ•°:
            output: è¾“å‡ºå†…å®¹
            context: æ‰§è¡Œä¸Šä¸‹æ–‡
            
        è¿”å›:
            bool: æ˜¯å¦éœ€è¦çº é”™
        """
        try:
            # ä½¿ç”¨ç®€åŒ–çš„å¿«é€Ÿè¯„ä¼°
            quick_prompt = f"""è¯·å¿«é€Ÿè¯„ä¼°ä»¥ä¸‹è¾“å‡ºæ˜¯å¦éœ€è¦çº é”™ï¼š

ä»»åŠ¡: {context.task_description}
è¾“å‡º: {output[:500]}...

è¯·åªå›ç­” "YES" æˆ– "NO"ï¼Œå¦‚æœéœ€è¦çº é”™å›ç­”YESï¼Œå¦åˆ™å›ç­”NOã€‚"""
            
            response = await self.llm.achat(quick_prompt)
            return "YES" in response.upper()
            
        except Exception as e:
            self.logger.error(f"å¿«é€Ÿè¯„ä¼°å¤±è´¥: {e}")
            return True  # é»˜è®¤éœ€è¦çº é”™

    def analyze_improvement_trend(self, 
                                history: List[CriticFeedback]) -> Dict[str, Any]:
        """
        åˆ†ææ”¹è¿›è¶‹åŠ¿
        
        å‚æ•°:
            history: å†å²åé¦ˆåˆ—è¡¨
            
        è¿”å›:
            Dict[str, Any]: æ”¹è¿›è¶‹åŠ¿åˆ†æ
        """
        if len(history) < 2:
            return {"trend": "insufficient_data", "improvement": 0.0}
        
        # è®¡ç®—è¯„åˆ†è¶‹åŠ¿
        scores = [feedback.overall_score for feedback in history]
        improvement = scores[-1] - scores[0]
        
        # åˆ¤æ–­è¶‹åŠ¿
        if improvement > 0.1:
            trend = "improving"
        elif improvement < -0.1:
            trend = "declining"
        else:
            trend = "stable"
        
        return {
            "trend": trend,
            "improvement": improvement,
            "current_score": scores[-1],
            "initial_score": scores[0],
            "score_history": scores
        }
